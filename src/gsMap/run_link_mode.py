import logging
import time
from pathlib import Path
import scanpy as sc
import pandas as pd

from gsMap.cauchy_combination_test import run_Cauchy_combination
from gsMap.config import GenerateLDScoreConfig, SpatialLDSCConfig, CauchyCombinationConfig, RunLinkModeConfig, ReportConfig
from gsMap.generate_ldscore import run_generate_ldscore
from gsMap.report import run_report
from gsMap.spatial_ldsc_multiple_sumstats import run_spatial_ldsc

def check_file(filepath):
    try:
        if filepath.endswith('.feather'):
            pd.read_feather(filepath)
        elif filepath.endswith('.h5ad'):
            sc.read_h5ad(filepath)
        elif filepath.endswith('.csv.gz'):
            pd.read_csv(filepath)
        print(f"{filepath} can be opened successfully")
        return True
    except Exception as e:
        print(f"{filepath} is corrupt")
        print(e)
        print(f"Regenerate {filepath}")
        return False

def format_duration(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    return f"{hours}h {minutes}m"


def run_pipeline_link(config: RunLinkModeConfig):
    # Set up logging
    log_file = Path(config.project_dir) / 'log' / f'{config.sample_name}_gsMap_pipeline.log'
    log_file.parent.mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format='[{asctime}] {levelname:.5s} | {name} - {message}',
        handlers=[
            logging.FileHandler(log_file),
        ],
        style='{'
    )

    logger = logging.getLogger('gsMap.pipeline.linking')
    logger.info("Starting pipeline with configuration: %s", config)

        
    ldscore_config = GenerateLDScoreConfig(
        workdir=config.workdir,
        project_name=config.project_name,
        sample_name=config.sample_name,
        chrom='all',
        bfile_root=config.bfile_root,
        keep_snp_root=config.keep_snp_root,
        gtf_annotation_file=config.gtffile,
        spots_per_chunk=1_000,
        baseline_annotation_dir=config.baseline_annotation_dir,
        SNP_gene_pair_dir=config.SNP_gene_pair_dir,
        ldscore_save_format='quick_mode',
        use_pooling = config.use_pooling,
    )

    pipeline_start_time = time.time()
    
        
    # Step 3: Generate LDScores
    start_time = time.time()
    logger.info("Step 3: Generating LDScores")

    # check if LDscore has been generated by the done file
    ldsc_done_file = Path(ldscore_config.ldscore_save_dir) / f"{config.sample_name}_generate_ldscore.done"
    if ldsc_done_file.exists():
        logger.info(f"Basic LDScore generation already done. Results saved at {ldscore_config.ldscore_save_dir}. Skipping...")
    else:
        run_generate_ldscore(ldscore_config)
        end_time = time.time()
        logger.info(f"Step 3 completed in {format_duration(end_time - start_time)}.")
        # create a done file
        ldsc_done_file.touch()

    # Step 4: Spatial LDSC
    start_time = time.time()
    logger.info("Step 4: Running spatial LDSC")

    sumstats_config = config.sumstats_config_dict
    for trait_name in sumstats_config:
        logger.info("Running spatial LDSC for trait: %s", trait_name)
        # detect if the spatial LDSC has been done:
        spatial_ldsc_result_file = Path(config.ldsc_save_dir) / f"{config.sample_name}_{trait_name}.csv.gz"

        if spatial_ldsc_result_file.exists() and check_file(spatial_ldsc_result_file.as_posix()):
            logger.info(
                f"Spatial LDSC already done for trait {trait_name}. Results saved at {spatial_ldsc_result_file}. Skipping...")
            continue

        spatial_ldsc_config_trait = SpatialLDSCConfig(
            workdir=config.workdir,
            project_name=config.project_name,
            sumstats_file=sumstats_config[trait_name],
            trait_name=trait_name,
            w_file=config.w_file,
            sample_name=config.sample_name,
            num_processes=config.max_processes,
            ldscore_save_format='quick_mode',
            spots_per_chunk_quick_mode=200,
            snp_gene_weight_adata_path=config.snp_gene_weight_adata_path,
            use_pooling = config.use_pooling,
        )
        from gsMap.spatial_ldsc_jax_final import run_spatial_ldsc_jax
        run_spatial_ldsc_jax(spatial_ldsc_config_trait)
        # run_spatial_ldsc(spatial_ldsc_config_trait)
    end_time = time.time()
    logger.info(f"Step 4 completed in {format_duration(end_time - start_time)}.")

    # Step 5: Cauchy combination test
    start_time = time.time()
    logger.info("Step 5: Running Cauchy combination test")
    '/storage/yangjianLab/chenwenhao/projects/202312_GPS/test/20240817_vanilla_pipeline_mouse_embryo_v4/E16.5_E1S1.MOSTA/cauchy_combination/E16.5_E1S1.MOSTA_Depression_2023_NatureMed.Cauchy.csv.gz'
    for trait_name in sumstats_config:
        # check if the cauchy combination has been done
        cauchy_result_file = config.get_cauchy_result_file(trait_name)
        if cauchy_result_file.exists() and check_file(cauchy_result_file.as_posix()):
            logger.info(
                f"Cauchy combination already done for trait {trait_name}. Results saved at {cauchy_result_file}. Skipping...")
            continue
        cauchy_config = CauchyCombinationConfig(
            workdir=config.workdir,
            project_name=config.project_name,
            sample_name=config.sample_name,
            annotation=config.annotation,
            trait_name=trait_name,
        )
        run_Cauchy_combination(cauchy_config)
    end_time = time.time()
    logger.info(f"Step 5 completed in {format_duration(end_time - start_time)}.")

    # # # Step 7: Generate final report
    # for trait_name in sumstats_config:
    #     logger.info("Running final report generation for trait: %s", trait_name)
    #     report_config = ReportConfig(
    #         workdir=config.workdir,
    #         project_name=config.project_name,
    #         sample_name=config.sample_name,
    #         annotation=config.annotation,
    #         trait_name=trait_name,
    #         plot_type='all',
    #         top_corr_genes=50,
    #         selected_genes=None,
    #         sumstats_file=sumstats_config[trait_name],
    #     )
    #     # Create the run parameters dictionary for each trait
    #     run_parameter_dict = {
    #         "Sample Name": config.sample_name,
    #         "Trait Name": trait_name,
    #         "Summary Statistics File": sumstats_config[trait_name],
    #         # "HDF5 Path": config.hdf5_path,
    #         "Annotation": config.annotation,
    #         "Number of Processes": config.max_processes,
    #         "Spatial LDSC Save Directory": config.ldsc_save_dir,
    #         "Cauchy Directory": config.cauchy_save_dir,
    #         "Report Directory": config.get_report_dir(trait_name),
    #         "gsMap Report File": config.get_gsMap_report_file(trait_name),
    #         "Gene Diagnostic Info File": config.get_gene_diagnostic_info_save_path(trait_name),
    #         "Spending Time": format_duration(time.time() - pipeline_start_time),
    #     }

    #     # Pass the run parameter dictionary to the report generation function
    #     run_report(report_config, run_parameters=run_parameter_dict)

    logger.info("Pipeline completed successfully.")
